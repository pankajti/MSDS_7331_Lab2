{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as mt\n",
    "import statsmodels.api as sm\n",
    "from time import time\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.classifier import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdefault=pd.read_csv('../data//UCI_Credit_Card.csv')\n",
    "ccdefault.rename(columns={'default.payment.next.month':'DEFAULT'},inplace=True)\n",
    "ccdefault.rename(columns={'PAY_0':'PAY_1'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdefault.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = ccdefault.copy()\n",
    "# Y Response variable dataframe\n",
    "mcc_Y = mcc['DEFAULT']\n",
    "# Features with no predictive features with respect to resposne variable\n",
    "mcredit = mcc.drop(['DEFAULT'],axis=1)\n",
    "# Create inci_X Explanatory Variables DF to support the individual models\n",
    "mcc_X = mcredit\n",
    "mcc_X_rf = mcredit.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = ccdefault.copy() # taking a copy in memory\n",
    "if 'DEFAULT' in cc:\n",
    "    y = cc['DEFAULT'].values\n",
    "    del cc['DEFAULT']\n",
    "    del cc['ID']\n",
    "    del cc['BILL_AMT1']\n",
    "    del cc['BILL_AMT2']\n",
    "    del cc['BILL_AMT3']\n",
    "    del cc['BILL_AMT4']\n",
    "    del cc['BILL_AMT5']\n",
    "    del cc['BILL_AMT6']\n",
    "    X = cc.values\n",
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "cv_object = StratifiedShuffleSplit(n_splits = num_cv_iterations,\n",
    "test_size = 0.20, train_size = 0.80, random_state=11)\n",
    "cv_object.get_n_splits(X, y)\n",
    "print(cv_object)\n",
    "for train_index, test_index in cv_object.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 3 \n",
    "We planned to use 3 models for our classification. Random forest , Decision tree and KNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest model \n",
    "\n",
    "Random forest is an ensempble technique that uses set of decision trees to handle classification problem. It uses random vectors to choose features absed on which it creates decision trees. Classification is done with each of these decision trees and then a majority vote is taken to decide final class. \n",
    "Following images gives an overview  of decision tree algo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url ='../images/random_forest_tan.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iter_num=0\n",
    "times_rec=[]\n",
    "decision_tree_classifiers = dict()\n",
    "random_forest_classifiers = dict()\n",
    "max_acc_rf= 0\n",
    "trn_idx_max_acc_rf = []\n",
    "tst_idx_max_acc_rf = []\n",
    "\n",
    "#for loop \n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices] \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "     #we count the time in executing the logistic regression\n",
    "    t0 = time()\n",
    "    random_forest = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                             random_state=0)\n",
    "\n",
    "    random_forest.fit(X_train,y_train)  # train object\n",
    "    y_hat = random_forest.predict(X_test) # get test set precitions\n",
    "    \n",
    "    t1=time()\n",
    "    diff=np.array([t1-t0])\n",
    "    \n",
    "    print (\"The time it takes to fit and predict for iteration {}  is {} \".format(iter_num, diff[0])) \n",
    "    times_rec=np.append(times_rec,diff)\n",
    "    \n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    if acc > max_acc_rf :\n",
    "        trn_idx_max_acc_rf = train_indices\n",
    "        tst_idx_max_acc_rf = test_indices\n",
    "        max_acc_rf = acc\n",
    "    \n",
    "    print (\"classification report for iteration {} \".format(iter_num))\n",
    "\n",
    "    rf_cl_reporter = ClassificationReport(random_forest, classes=['non_default' , 'default'], support=True)\n",
    "\n",
    "    rf_cl_reporter.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "    rf_cl_reporter.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "    rf_cl_reporter.show()   \n",
    "    \n",
    "    pr_curve = PrecisionRecallCurve(random_forest)\n",
    "    pr_curve.fit(X_train, y_train)\n",
    "    pr_curve.score(X_test, y_test)\n",
    "    pr_curve.show()\n",
    "    \n",
    "    \n",
    "    roc_curve = ROCAUC(random_forest, classes=[\"not_default\", \"default\"])\n",
    "    roc_curve.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "    roc_curve.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "    roc_curve.show() \n",
    "    \n",
    "\n",
    "    cm = ConfusionMatrix(random_forest, classes=['not default' , 'default'])\n",
    "    # Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "    cm.fit(X_train, y_train)\n",
    "    # To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "    # and then creates the confusion_matrix from scikit-learn.\n",
    "    cm.score(X_test, y_test)\n",
    "    # How did we do?\n",
    "    cm.show()\n",
    "     \n",
    "    iter_num+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest with class_weight = balanced \n",
    "In above section we explored random forest with default parameters. Default parameters has 100 estimators and max_depth value 2 also used default class weight wich gives equal weight to each of the classes. However in the cases where claases are unbalanced we need to adjust weight of the classes so that they are properly represented. for example in our case we have two classes default and non default and each class will get equal weight. However "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "random_forest = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                         random_state=0, class_weight = 'balanced')\n",
    "\n",
    "random_forest.fit(X_train,y_train)  # train object\n",
    "y_hat = random_forest.predict(X_test) # get test set precitions\n",
    "\n",
    "t1=time()\n",
    "diff=np.array([t1-t0])\n",
    "\n",
    "print (\"The time it takes to fit and predict for iteration {}  is {} \".format(iter_num, diff[0])) \n",
    "times_rec=np.append(times_rec,diff)\n",
    "\n",
    "\n",
    "# now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print (\"classification report for iteration {} \".format(iter_num))\n",
    "\n",
    "rf_cl_reporter = ClassificationReport(random_forest, classes=['non_default' , 'default'], support=True)\n",
    "\n",
    "rf_cl_reporter.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "rf_cl_reporter.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "rf_cl_reporter.show()   \n",
    "\n",
    "pr_curve = PrecisionRecallCurve(random_forest)\n",
    "pr_curve.fit(X_train, y_train)\n",
    "pr_curve.score(X_test, y_test)\n",
    "pr_curve.show()\n",
    "\n",
    "\n",
    "roc_curve = ROCAUC(random_forest, classes=[\"not_default\", \"default\"])\n",
    "roc_curve.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "roc_curve.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "roc_curve.show() \n",
    "\n",
    "\n",
    "cm = ConfusionMatrix(random_forest, classes=['not default' , 'default'])\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "# How did we do?\n",
    "cm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t1=time()\n",
    "diff=np.array([t1-t0])\n",
    "\n",
    "print (\"The time it takes to fit and predict for iteration {}  is {} \".format(iter_num, diff[0])) \n",
    "times_rec=np.append(times_rec,diff)\n",
    "\n",
    "\n",
    "# now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print (\"classification report for iteration {} \".format(iter_num))\n",
    "\n",
    "fig = plt.figure()\n",
    "#ax = fig.add_subplot(211)\n",
    "\n",
    "rf_cl_reporter = ClassificationReport(random_forest, classes=['non_default' , 'default'], support=True)\n",
    "rf_cl_reporter.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "rf_cl_reporter.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "rf_cl_reporter.show()   \n",
    "\n",
    "pr_curve = PrecisionRecallCurve(random_forest)\n",
    "pr_curve.fit(X_train, y_train)\n",
    "pr_curve.score(X_test, y_test)\n",
    "pr_curve.show()\n",
    "\n",
    "\n",
    "roc_curve = ROCAUC(random_forest, classes=[\"not_default\", \"default\"])\n",
    "roc_curve.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "roc_curve.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "roc_curve.show() \n",
    "\n",
    "\n",
    "cm = ConfusionMatrix(random_forest, classes=['not default' , 'default'])\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "# How did we do?\n",
    "cm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "random_forest = RandomForestClassifier(n_estimators=100, max_depth=5,\n",
    "                         random_state=0, class_weight = 'balanced')\n",
    "\n",
    "random_forest.fit(X_train,y_train)  # train object\n",
    "y_hat = random_forest.predict(X_test) # get test set precitions\n",
    "\n",
    "t1=time()\n",
    "diff=np.array([t1-t0])\n",
    "\n",
    "print (\"The time it takes to fit and predict for iteration {}  is {} \".format(iter_num, diff[0])) \n",
    "times_rec=np.append(times_rec,diff)\n",
    "\n",
    "\n",
    "# now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print (\"classification report for iteration {} \".format(iter_num))\n",
    "\n",
    "rf_cl_reporter = ClassificationReport(random_forest, classes=['non_default' , 'default'], support=True)\n",
    "\n",
    "rf_cl_reporter.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "rf_cl_reporter.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "rf_cl_reporter.show()   \n",
    "\n",
    "pr_curve = PrecisionRecallCurve(random_forest)\n",
    "pr_curve.fit(X_train, y_train)\n",
    "pr_curve.score(X_test, y_test)\n",
    "pr_curve.show()\n",
    "\n",
    "\n",
    "roc_curve = ROCAUC(random_forest, classes=[\"not_default\", \"default\"])\n",
    "roc_curve.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "roc_curve.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "roc_curve.show() \n",
    "\n",
    "\n",
    "cm = ConfusionMatrix(random_forest, classes=['not default' , 'default'])\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "# How did we do?\n",
    "cm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Decision Tree classifiers \n",
    "\n",
    "We need to create some more  models by adjusting n_estimators and max_depth parameters. Also we need to  create one models with class_weight='balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url ='../images/decision_tree_tan.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iter_num=0\n",
    "times_rec=[]\n",
    "decision_tree_classifiers = dict()\n",
    "random_forest_classifiers = dict()\n",
    "max_acc_dt = 0\n",
    "trn_idx_max_acc_dt =[]\n",
    "tst_idx_max_acc_dt = []\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "     #we count the time in executing the logistic regression\n",
    "    t0 = time()\n",
    "     \n",
    "    decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "    decision_tree.fit(X_train,y_train)  # train object\n",
    "    y_hat = decision_tree.predict(X_test) # get test set precitions\n",
    "    \n",
    "    t1=time()\n",
    "    diff=np.array([t1-t0])\n",
    "    \n",
    "    print (\"The time it takes to fit and predict for iteration {}  is {} \".format(iter_num, diff[0])) \n",
    "    times_rec=np.append(times_rec,diff)\n",
    "    \n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    if acc> max_acc_dt:\n",
    "        max_acc_dt = acc\n",
    "        trn_idx_max_acc_dt = train_indices\n",
    "        tst_idx_max_acc_dt = test_indices\n",
    "        \n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print (\"classification report for iteration {} \".format(iter_num))\n",
    "    \n",
    "    \n",
    "    dt_cl_reporter = ClassificationReport(decision_tree, classes=['non_default' , 'default'], support=True)\n",
    "\n",
    "    dt_cl_reporter.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "    dt_cl_reporter.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "    dt_cl_reporter.show()  \n",
    "    \n",
    "    pr_curve = PrecisionRecallCurve(decision_tree)\n",
    "    pr_curve.fit(X_train, y_train)\n",
    "    pr_curve.score(X_test, y_test)\n",
    "    pr_curve.show()\n",
    "    \n",
    "    roc_curve = ROCAUC(decision_tree, classes=[\"not_default\", \"default\"])\n",
    "\n",
    "    roc_curve.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "    roc_curve.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "    roc_curve.show() \n",
    "    \n",
    "    cm = ConfusionMatrix(decision_tree, classes=['not default' , 'default'])\n",
    "    # Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "    cm.fit(X_train, y_train)\n",
    "    # To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "    # and then creates the confusion_matrix from scikit-learn.\n",
    "    cm.score(X_test, y_test)\n",
    "    # How did we do?\n",
    "    cm.show()\n",
    "    \n",
    "        \n",
    "    iter_num+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree with balanced class \n",
    "class weight impacts decison tree classifier as well and we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "     \n",
    "decision_tree = DecisionTreeClassifier(class_weight = 'balanced')\n",
    "\n",
    "decision_tree.fit(X_train,y_train)  # train object\n",
    "y_hat = decision_tree.predict(X_test) # get test set precitions\n",
    "\n",
    "t1=time()\n",
    "diff=np.array([t1-t0])\n",
    "print (\"The time it takes to fit and predict for iteration {}  is {} \".format(iter_num, diff[0])) \n",
    "times_rec=np.append(times_rec,diff)\n",
    "# now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "print(\"accurancy for balanced class is {}\".format(acc))\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print (\"classification report for iteration {} \".format(iter_num))\n",
    "dt_cl_reporter = ClassificationReport(decision_tree, classes=['non_default' , 'default'], support=True)\n",
    "\n",
    "dt_cl_reporter.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "dt_cl_reporter.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "dt_cl_reporter.show()  \n",
    "\n",
    "pr_curve = PrecisionRecallCurve(decision_tree)\n",
    "pr_curve.fit(X_train, y_train)\n",
    "pr_curve.score(X_test, y_test)\n",
    "pr_curve.show()\n",
    "\n",
    "roc_curve = ROCAUC(decision_tree, classes=[\"not_default\", \"default\"])\n",
    "\n",
    "roc_curve.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "roc_curve.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "roc_curve.show() \n",
    "\n",
    "cm = ConfusionMatrix(decision_tree, classes=['not default' , 'default'])\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "# How did we do?\n",
    "cm.show()\n",
    "\n",
    "iter_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN \n",
    "TBD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = [10, 20, 50, 100]\n",
    "max_acc=0\n",
    "for K in parameters:\n",
    "    print(\"For k = \", K, \"and metric = Euclidean: \")\n",
    "    knn = KNeighborsClassifier(n_neighbors=K, weights='uniform', metric='euclidean')\n",
    "    knn.fit(X_train_scaled,y_train)\n",
    "    y_hat = knn.predict(X_test_scaled)\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print('accuracy:', acc )\n",
    "    print(conf)\n",
    "    ClassReport = mt.classification_report(y_test,y_hat)\n",
    "    print(ClassReport)\n",
    "    max_acc = (acc) if acc>max_acc else (max_acc)\n",
    "    \n",
    "    print (\"classification report for iteration {} \".format(iter_num))\n",
    "    knn_cl_reporter = ClassificationReport(knn, classes=['non_default' , 'default'], support=True)\n",
    "\n",
    "    knn_cl_reporter.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "    knn_cl_reporter.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "    knn_cl_reporter.show()  \n",
    "\n",
    "    pr_curve = PrecisionRecallCurve(decision_tree)\n",
    "    pr_curve.fit(X_train, y_train)\n",
    "    pr_curve.score(X_test, y_test)\n",
    "    pr_curve.show()\n",
    "\n",
    "    roc_curve = ROCAUC(decision_tree, classes=[\"not_default\", \"default\"])\n",
    "\n",
    "    roc_curve.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "    roc_curve.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "    roc_curve.show() \n",
    "\n",
    "    cm = ConfusionMatrix(decision_tree, classes=['not default' , 'default'])\n",
    "    # Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "    cm.fit(X_train, y_train)\n",
    "    # To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "    # and then creates the confusion_matrix from scikit-learn.\n",
    "    cm.score(X_test, y_test)\n",
    "    # How did we do?\n",
    "    cm.show()\n",
    "    \n",
    "#print(max_acc)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 4\n",
    "We can give some details from the graphs created in previous section and explain those "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest \n",
    "Random Forest gives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_idx_max_acc_dt\n",
    "tst_idx_max_acc_dt\n",
    "X_train = X[trn_idx_max_acc_dt]\n",
    "y_train = y[trn_idx_max_acc_dt]\n",
    "X_test = X[tst_idx_max_acc_dt]\n",
    "y_test = y[tst_idx_max_acc_dt]\n",
    "\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(class_weight = 'balanced')\n",
    "\n",
    "decision_tree.fit(X_train,y_train)  # train object\n",
    "y_hat = decision_tree.predict(X_test) # get test set precitions\n",
    "\n",
    "t1=time()\n",
    "diff=np.array([t1-t0])\n",
    "print (\"The time it takes to fit and predict for iteration {}  is {} \".format(iter_num, diff[0])) \n",
    "times_rec=np.append(times_rec,diff)\n",
    "# now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "print(\"accurancy for balanced class is {}\".format(acc))\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print (\"classification report for iteration {} \".format(iter_num))\n",
    "dt_cl_reporter = ClassificationReport(decision_tree, classes=['non_default' , 'default'], support=True)\n",
    "\n",
    "dt_cl_reporter.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "dt_cl_reporter.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "dt_cl_reporter.show()  \n",
    "\n",
    "pr_curve = PrecisionRecallCurve(decision_tree)\n",
    "pr_curve.fit(X_train, y_train)\n",
    "pr_curve.score(X_test, y_test)\n",
    "pr_curve.show()\n",
    "\n",
    "roc_curve = ROCAUC(decision_tree, classes=[\"not_default\", \"default\"])\n",
    "\n",
    "roc_curve.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "roc_curve.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "roc_curve.show() \n",
    "\n",
    "cm = ConfusionMatrix(decision_tree, classes=['not default' , 'default'])\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "# How did we do?\n",
    "cm.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iter_num=0\n",
    "times_rec=[]\n",
    "decision_tree_classifiers = dict()\n",
    "random_forest_classifiers = dict()\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices] \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "     #we count the time in executing the logistic regression\n",
    "    t0 = time()\n",
    "    random_forest = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                             random_state=0)\n",
    "    viz = FeatureImportances(random_forest, labels = cc.columns)\n",
    "    viz.fit(X_train_scaled,y_train)\n",
    "    viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iter_num=0\n",
    "times_rec=[]\n",
    "decision_tree_classifiers = dict()\n",
    "random_forest_classifiers = dict()\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices] \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "     #we count the time in executing the logistic regression\n",
    "    t0 = time()\n",
    "    random_forest = DecisionTreeClassifier()\n",
    "    viz = FeatureImportances(decision_tree, labels = cc.columns)\n",
    "    viz.fit(X_train_scaled,y_train)\n",
    "    viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN \n",
    "\n",
    "In case of knn there is no output for feature importance. We run into following error when we attempt to draw graph in same way in which we did for decision tree and random forest. \n",
    "In case of KNN we use particular metric to determine proximity of nodes and cluster them accordingly. The distnace is measured based on the difference between corresponding features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num=0\n",
    "times_rec=[]\n",
    " \n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices] \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "     #we count the time in executing the logistic regression\n",
    "    t0 = time()\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='euclidean')\n",
    "    viz = FeatureImportances(knn, labels = cc.columns)\n",
    "    viz.fit(X_train_scaled,y_train)\n",
    "    viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 6\n",
    "\n",
    "Following is a Comparison of Decision Tree and Random forest classifiers with same date. We are using error estimates to compare two models. The error estimates should follow Gaussian distribution. We calculate mean and standard error using the formula discussed in the class. \n",
    "<TODO add image and formula >\n",
    "    We are caclulating confidence interval for error differences to figure out if the differences are significant\n",
    "  We can add similar test for knn , decision tree and knn and random forest comparison \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree and Random Forest Comparision based on error rate \n",
    "\n",
    "Our model comparison is based on following formula. \n",
    "We compare d bar by taking averedge  of the differences of the error rates of the models in different folds using same training set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url ='../images/model_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num=0\n",
    "times_rec=[]\n",
    "error_rec= [] \n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    decision_tree = DecisionTreeClassifier()\n",
    "    decision_tree.fit(X_train,y_train) # train object\n",
    "    \n",
    "    random_forest = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                             random_state=0)\n",
    "    random_forest.fit(X_train,y_train) \n",
    "    \n",
    "    y_hat_dt = decision_tree.predict(X_test) \n",
    "    y_hat_rf = random_forest.predict(X_test) \n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc_dt = mt.accuracy_score(y_test,y_hat_dt)\n",
    "    acc_rf = mt.accuracy_score(y_test,y_hat_rf)\n",
    "    error_rec.append(acc_dt - acc_rf)\n",
    "\n",
    "    iter_num+=1\n",
    "\n",
    "print(error_rec)\n",
    "\n",
    "d_bar = sum(error_rec)/len(error_rec)\n",
    "siqma_squared = sum((error_rec - d_bar))/(5-1)\n",
    "print(d_bar)\n",
    "print(siqma_squared)\n",
    "\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "confidence_interval = ( d_bar + (1/math.sqrt(4))*stats.t.isf(.975, 4),  d_bar - (1/math.sqrt(4))*stats.t.isf(.975, 4))\n",
    "print(confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above tht zero is a possible value for 95% confidence interval it means that we can't reject the null hypothesis that both the models have no difference in their error rate with 95% confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  KNN  and Random Forest Comparision based on error rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iter_num=0\n",
    "times_rec=[]\n",
    "error_rec= [] \n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='euclidean')\n",
    "    knn.fit(X_train,y_train) # train object\n",
    "    \n",
    "    random_forest = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                             random_state=0)\n",
    "    random_forest.fit(X_train,y_train) \n",
    "    \n",
    "    y_hat_knn = knn.predict(X_test) \n",
    "    y_hat_rf = random_forest.predict(X_test) \n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc_knn = mt.accuracy_score(y_test,y_hat_knn)\n",
    "    acc_rf = mt.accuracy_score(y_test,y_hat_rf)\n",
    "    error_rec.append(acc_knn - acc_rf)\n",
    "\n",
    "    iter_num+=1\n",
    "\n",
    "print(error_rec)\n",
    "\n",
    "d_bar = sum(error_rec)/len(error_rec)\n",
    "siqma_squared = sum((error_rec - d_bar))/(5-1)\n",
    "print(d_bar)\n",
    "print(siqma_squared)\n",
    "\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "confidence_interval = ( d_bar + (1/math.sqrt(4))*stats.t.isf(.975, 4),  d_bar - (1/math.sqrt(4))*stats.t.isf(.975, 4))\n",
    "print(confidence_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
